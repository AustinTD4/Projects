{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./Fourrier/')\n",
    "print(files)\n",
    "files2 = []\n",
    "for i in range(int(len(files)/10)):\n",
    "    files2.append(files[i*10])\n",
    "print(files2)\n",
    "Viable_Cols = ['Acceleration x (m/s^2)_kalman', 'Acceleration y (m/s^2)_kalman', 'Acceleration z (m/s^2)_kalman', 'Gyroscope x (rad/s)_kalman',\n",
    "              'Gyroscope y (rad/s)_kalman', 'Gyroscope z (rad/s)_kalman', 'Linear Acceleration x (m/s^2)_kalman',\n",
    "              'Linear Acceleration y (m/s^2)_kalman', 'Linear Acceleration z (m/s^2)_kalman']\n",
    "Viable_Cols2 = []\n",
    "Viable_Cols3 = copy.deepcopy(Viable_Cols)\n",
    "for column in Viable_Cols:\n",
    "    newCol = str(column+'_max_freq')\n",
    "    Viable_Cols2.append(newCol)\n",
    "    Viable_Cols3.append(newCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "slide = 0.1\n",
    "train_test = 0.8\n",
    "def make_windows(window, slide, frame, train_test):\n",
    "    jump = window*slide\n",
    "    train_samples = []\n",
    "    sparse_train = []\n",
    "    test_samples = []\n",
    "    sparse_test = []\n",
    "    for i in range(int(len(frame['Acceleration x (m/s^2)_kalman'])/(window*slide))-int((1/slide))):\n",
    "        #print(i*20+200)\n",
    "        sample = frame.loc[(i*20):(i*20)+199, Viable_Cols]\n",
    "        sample2 = []\n",
    "        freqs = frame.loc[(i*20)+199, Viable_Cols2]\n",
    "        sample2.append(freqs.tolist())\n",
    "        mins = [np.min(sample[col]) for col in Viable_Cols]\n",
    "        maxes = [np.max(sample[col]) for col in Viable_Cols]\n",
    "        stdev = [np.std(sample[col], axis=0) for col in Viable_Cols]\n",
    "        sample2.append(mins)\n",
    "        sample2.append(maxes)\n",
    "        sample2.append(stdev)\n",
    "        if i*20 <= len(frame['Acceleration x (m/s^2)_kalman'])*train_test:\n",
    "            train_samples.append(sample)\n",
    "            sparse_train.append(sample2)\n",
    "        else:\n",
    "            test_samples.append(sample)\n",
    "            sparse_test.append(sample2)\n",
    "        \n",
    "        test_samples = test_samples[9:]\n",
    "        sparse_test = sparse_test[9:]\n",
    "    return train_samples, sparse_train, test_samples, sparse_test\n",
    "frame = pd.read_csv('./Fourrier/Fourrier Austin Biking 60rpm.csv')\n",
    "print(int(len(frame['Acceleration x (m/s^2)_kalman'])/(window*slide))-10)\n",
    "print(len(frame['Acceleration x (m/s^2)']))\n",
    "print(window*slide)\n",
    "#train_samples, sparse_train, test_samples, sparse_test = make_windows(window, slide, frame, train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "slide = 0.1\n",
    "train_test = 0.8\n",
    "def make_windows_PCA(window, slide, frame, train_test):\n",
    "    Viable_Cols = ['0', '1', '2', '3', '4', '5']\n",
    "    Viable_Cols2 = ['0_max_freq', '1_max_freq', '2_max_freq', '3_max_freq', '4_max_freq', '5_max_freq', ]\n",
    "    jump = window*slide\n",
    "    train_samples = []\n",
    "    sparse_train = []\n",
    "    test_samples = []\n",
    "    sparse_test = []\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "    if 'SPM/RPM' in frame.columns:\n",
    "        label = 'SPM/RPM'\n",
    "    if 'RPM/SPM' in frame.columns:\n",
    "        label = 'RPM/SPM'\n",
    "    if 'RPM' in frame.columns:\n",
    "        label = 'RPM'\n",
    "    if 'SPM' in frame.columns:\n",
    "        label = 'SPM'\n",
    "    for i in range(int(len(frame['0'])/(window*slide))-int((1/slide))):\n",
    "        sample = frame.loc[(i*jump)+1:(i*jump)+(window), Viable_Cols]\n",
    "        sample2 = []\n",
    "        freqs = frame.loc[(i*jump)+200, Viable_Cols2]\n",
    "        sample2.append(freqs.tolist())\n",
    "        mins = [np.min(sample[col]) for col in Viable_Cols]\n",
    "        maxes = [np.max(sample[col]) for col in Viable_Cols]\n",
    "        stdev = [np.std(sample[col], axis=0) for col in Viable_Cols]\n",
    "        means = [np.mean(sample[col]) for col in Viable_Cols]\n",
    "        sample2.append(mins)\n",
    "        sample2.append(maxes)\n",
    "        sample2.append(stdev)\n",
    "        sample2.append(means)\n",
    "        yval = frame[label][(i*jump)+199]\n",
    "        if i*jump <= (len(frame['0'])*train_test):\n",
    "            train_samples.append(sample)\n",
    "            sparse_train.append(sample2)\n",
    "            labels_train.append(yval)\n",
    "        else:\n",
    "            test_samples.append(sample)\n",
    "            sparse_test.append(sample2)\n",
    "            labels_test.append(yval)\n",
    "    labels_test = labels_test[9:]\n",
    "    test_samples = test_samples[9:]\n",
    "    sparse_test = sparse_test[9:]\n",
    "    return train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./PCA Fourier/')\n",
    "bike_train_samples, bike_sparse_train, bike_test_samples, bike_sparse_test, bike_labels_train, bike_labels_test = [],[],[],[],[],[]\n",
    "walk_train_samples, walk_sparse_train, walk_test_samples, walk_sparse_test, walk_labels_train, walk_labels_test = [],[],[],[],[],[]\n",
    "run_train_samples, run_sparse_train, run_test_samples, run_sparse_test, run_labels_train, run_labels_test = [],[],[],[],[],[]\n",
    "\n",
    "Biking_Files = [0,1,2,16,17]\n",
    "for i in Biking_Files:\n",
    "    print('A')\n",
    "    frame = pd.read_csv('./PCA Fourier/'+str(files[i]))\n",
    "    train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test = make_windows_PCA(window, slide, frame, train_test)\n",
    "    bike_train_samples.extend(train_samples)\n",
    "    bike_sparse_train.extend(sparse_train)\n",
    "    bike_test_samples.extend(test_samples)\n",
    "    bike_sparse_test.extend(sparse_test)\n",
    "    bike_labels_train.extend(labels_train)\n",
    "    bike_labels_test.extend(labels_test)\n",
    "Walking_Files = [10,11,12,13,14,18,20,21]\n",
    "for i in Walking_Files:\n",
    "    print('B')\n",
    "    frame = pd.read_csv('./PCA Fourier/'+str(files[i]))\n",
    "    train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test = make_windows_PCA(window, slide, frame, train_test)\n",
    "    walk_train_samples.extend(train_samples)\n",
    "    walk_sparse_train.extend(sparse_train)\n",
    "    walk_test_samples.extend(test_samples)\n",
    "    walk_sparse_test.extend(sparse_test)\n",
    "    walk_labels_train.extend(labels_train)\n",
    "    walk_labels_test.extend(labels_test)\n",
    "Running_Files = [4,5,6,7]\n",
    "for i in Running_Files:\n",
    "    print('C')\n",
    "    frame = pd.read_csv('./PCA Fourier/'+str(files[i]))\n",
    "    train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test = make_windows_PCA(window, slide, frame, train_test)\n",
    "    run_train_samples.extend(train_samples)\n",
    "    run_sparse_train.extend(sparse_train)\n",
    "    run_test_samples.extend(test_samples)\n",
    "    run_sparse_test.extend(sparse_test)\n",
    "    run_labels_train.extend(labels_train)\n",
    "    run_labels_test.extend(labels_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(run_train_samples))\n",
    "print(len(walk_train_samples))\n",
    "print(len(bike_train_samples))\n",
    "print(len(run_test_samples))\n",
    "print(len(walk_test_samples))\n",
    "print(len(bike_test_samples))\n",
    "print(len(bike_labels_test))\n",
    "print(run_train_samples[1])\n",
    "print(run_sparse_train[1])\n",
    "\n",
    "def reshaper(frame):\n",
    "    data_2d = frame.reshape(-1, frame.shape[-1])\n",
    "    scaler = StandardScaler()\n",
    "    whitened_data_2d = scaler.fit_transform(data_2d)\n",
    "    whitened_frame = whitened_data_2d.reshape(frame.shape)\n",
    "    return whitened_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 48.2801 - mean_absolute_error: 48.2801\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 0s 931us/step - loss: 17.0630 - mean_absolute_error: 17.0630\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 0s 981us/step - loss: 14.3509 - mean_absolute_error: 14.3509\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 0s 930us/step - loss: 14.0258 - mean_absolute_error: 14.0258\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 0s 930us/step - loss: 11.2394 - mean_absolute_error: 11.2394\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 11.6860 - mean_absolute_error: 11.6860\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 0s 915us/step - loss: 11.5575 - mean_absolute_error: 11.5575\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 0s 965us/step - loss: 10.9047 - mean_absolute_error: 10.9047\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 0s 874us/step - loss: 11.9086 - mean_absolute_error: 11.9086\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 0s 934us/step - loss: 10.2030 - mean_absolute_error: 10.2030\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 6.5075 - mean_absolute_error: 6.5075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.507479190826416, 6.507479190826416]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_run_sparse_train = np.array(run_sparse_train)\n",
    "tf_run_labels_train = np.array(run_labels_train)\n",
    "tf_run_sparse_test = np.array(run_sparse_test)\n",
    "tf_run_labels_test = np.array(run_labels_test)\n",
    "\n",
    "whitened_run_sparse_train = reshaper(tf_run_sparse_train)\n",
    "whitened_run_sparse_test = reshaper(tf_run_sparse_test)\n",
    "\n",
    "model_run = Sequential([\n",
    "    Flatten(input_shape=(5,6)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='relu')])\n",
    "model_run.compile(optimizer=Adam(learning_rate=0.02), loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "model_run.fit(whitened_run_sparse_train, tf_run_labels_train, epochs=10)\n",
    "model_run.evaluate(whitened_run_sparse_test, tf_run_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 0s 705us/step - loss: 18.8868 - mean_absolute_error: 18.8868\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 0s 678us/step - loss: 10.0861 - mean_absolute_error: 10.0861\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 0s 675us/step - loss: 11.3521 - mean_absolute_error: 11.3521\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 0s 698us/step - loss: 8.2101 - mean_absolute_error: 8.2101\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 0s 745us/step - loss: 8.1675 - mean_absolute_error: 8.1675\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 0s 683us/step - loss: 8.5796 - mean_absolute_error: 8.5796\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 0s 675us/step - loss: 7.2221 - mean_absolute_error: 7.2221\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 0s 698us/step - loss: 6.7111 - mean_absolute_error: 6.7111\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 0s 690us/step - loss: 6.8318 - mean_absolute_error: 6.8318\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 0s 683us/step - loss: 6.1215 - mean_absolute_error: 6.1215\n",
      "12/12 [==============================] - 0s 672us/step - loss: 6.6386 - mean_absolute_error: 6.6386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.638564109802246, 6.638564109802246]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_walk_sparse_train = np.array(walk_sparse_train)\n",
    "tf_walk_labels_train = np.array(walk_labels_train)\n",
    "tf_walk_sparse_test = np.array(walk_sparse_test)\n",
    "tf_walk_labels_test = np.array(walk_labels_test)\n",
    "\n",
    "whitened_walk_sparse_train = reshaper(tf_walk_sparse_train)\n",
    "whitened_walk_sparse_test = reshaper(tf_walk_sparse_test)\n",
    "\n",
    "model_walk = Sequential([\n",
    "    Flatten(input_shape=(5,6)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='relu')])\n",
    "model_walk.compile(optimizer=Adam(learning_rate=0.1), loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "model_walk.fit(whitened_walk_sparse_train, tf_walk_labels_train, epochs=10)\n",
    "model_walk.evaluate(whitened_walk_sparse_test, tf_walk_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 994us/step - loss: 23.6873 - mean_absolute_error: 23.6873\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 926us/step - loss: 14.5660 - mean_absolute_error: 14.5660\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 926us/step - loss: 12.7625 - mean_absolute_error: 12.7625\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 998us/step - loss: 12.9078 - mean_absolute_error: 12.9078\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 925us/step - loss: 10.8927 - mean_absolute_error: 10.8927\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 925us/step - loss: 9.7742 - mean_absolute_error: 9.7742\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 961us/step - loss: 10.3069 - mean_absolute_error: 10.3069\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 908us/step - loss: 10.1552 - mean_absolute_error: 10.1552\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 997us/step - loss: 11.6537 - mean_absolute_error: 11.6537\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 961us/step - loss: 10.7233 - mean_absolute_error: 10.7233\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 12.7979 - mean_absolute_error: 12.7979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.79792594909668, 12.79792594909668]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_bike_sparse_train = np.array(bike_sparse_train)\n",
    "tf_bike_labels_train = np.array(bike_labels_train)\n",
    "tf_bike_sparse_test = np.array(bike_sparse_test)\n",
    "tf_bike_labels_test = np.array(bike_labels_test)\n",
    "\n",
    "whitened_bike_sparse_train = reshaper(tf_bike_sparse_train)\n",
    "whitened_bike_sparse_test = reshaper(tf_bike_sparse_test)\n",
    "\n",
    "model_bike = Sequential([\n",
    "    Flatten(input_shape=(5,6)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='relu')])\n",
    "model_bike.compile(optimizer=Adam(learning_rate=0.1), loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "model_bike.fit(whitened_bike_sparse_train, tf_bike_labels_train, epochs=10)\n",
    "model_bike.evaluate(whitened_bike_sparse_test, tf_bike_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows_full_dataset(window, slide, frame, train_test):\n",
    "    Viable_Cols = ['0', '1', '2', '3', '4', '5']\n",
    "    Viable_Cols2 = ['0_max_freq', '1_max_freq', '2_max_freq', '3_max_freq', '4_max_freq', '5_max_freq', ]\n",
    "    jump = window*slide\n",
    "    train_samples = []\n",
    "    sparse_train = []\n",
    "    test_samples = []\n",
    "    sparse_test = []\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "    for i in range(int(len(frame['0'])/(window*slide))-int((1/slide))):\n",
    "        sample = frame.loc[(i*jump):(i*jump)+(window-1), Viable_Cols]\n",
    "        sample2 = []\n",
    "        freqs = frame.loc[(i*jump)+199, Viable_Cols2]\n",
    "        sample2.append(freqs.tolist())\n",
    "        mins = [np.min(sample[col]) for col in Viable_Cols]\n",
    "        maxes = [np.max(sample[col]) for col in Viable_Cols]\n",
    "        stdev = [np.std(sample[col], axis=0) for col in Viable_Cols]\n",
    "        means = [np.mean(sample[col]) for col in Viable_Cols]\n",
    "        sample2.append(mins)\n",
    "        sample2.append(maxes)\n",
    "        sample2.append(stdev)\n",
    "        sample2.append(means)\n",
    "        if frame['Still'][(i*jump)+199] == 1:\n",
    "            yval = 0\n",
    "        if frame['Walk'][(i*jump)+199] == 1:\n",
    "            yval = 1\n",
    "        if frame['Run'][(i*jump)+199] == 1:\n",
    "            yval = 2\n",
    "        if frame['Cycle'][(i*jump)+199] == 1:\n",
    "            yval = 3\n",
    "        \n",
    "        if i*jump <= (len(frame['0'])*train_test):\n",
    "            train_samples.append(sample)\n",
    "            sparse_train.append(sample2)\n",
    "            labels_train.append(yval)\n",
    "        else:\n",
    "            test_samples.append(sample)\n",
    "            sparse_test.append(sample2)\n",
    "            labels_test.append(yval)\n",
    "    labels_test = labels_test[9:]\n",
    "    test_samples = test_samples[9:]\n",
    "    sparse_test = sparse_test[9:]\n",
    "    return train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test\n",
    "\n",
    "def sampler(train_samples, sparse_train, labels_train, num):\n",
    "    length = len(train_samples)\n",
    "    ids = random.sample(range(length), num)\n",
    "    dataSamples = []\n",
    "    sparseSamples = []\n",
    "    labelSamples = []\n",
    "    for i in ids:\n",
    "        dataSamples.append(train_samples[i])\n",
    "        sparseSamples.append(sparse_train[i])\n",
    "        labelSamples.append(labels_train[i])\n",
    "    return dataSamples, sparseSamples, labelSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('Complete_dataset_PCA_Fourier.csv')\n",
    "train_samples, sparse_train, test_samples, sparse_test, labels_train, labels_test = make_windows_full_dataset(window, slide, frame, train_test)\n",
    "num = 20\n",
    "dataSamples, sparseSamples, labelSamples = sampler(train_samples, sparse_train, labels_train, num)\n",
    "tf_sparse_train = np.array(sparse_train)\n",
    "tf_labels_train = np.array(labels_train)\n",
    "tf_sparse_test = np.array(sparse_test)\n",
    "tf_labels_test = np.array(labels_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5792, 5, 6)\n",
      "Epoch 1/5\n",
      "181/181 [==============================] - 1s 1ms/step - loss: 0.2773 - accuracy: 0.8954\n",
      "Epoch 2/5\n",
      "181/181 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9287\n",
      "Epoch 3/5\n",
      "181/181 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9396\n",
      "Epoch 4/5\n",
      "181/181 [==============================] - 0s 991us/step - loss: 0.1220 - accuracy: 0.9361\n",
      "Epoch 5/5\n",
      "181/181 [==============================] - 0s 998us/step - loss: 0.1177 - accuracy: 0.9430\n",
      "45/45 [==============================] - 0s 638us/step - loss: 0.3613 - accuracy: 0.9053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3613187372684479, 0.9053295850753784]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf_sparse_train.shape)\n",
    "whitened_tf_sparse_train = reshaper(tf_sparse_train[1:])\n",
    "\n",
    "model_classify = Sequential([\n",
    "    Flatten(input_shape=(5,6)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(4, activation='softmax')])\n",
    "model_classify.compile(optimizer=Adam(learning_rate=0.005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_classify.fit(whitened_tf_sparse_train, tf_labels_train[1:], epochs=5)\n",
    "whitened_tf_sparse_test = reshaper(tf_sparse_test[1:])\n",
    "model_classify.evaluate(whitened_tf_sparse_test, tf_labels_test[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 0s 578us/step\n",
      "87/87 [==============================] - 0s 504us/step\n",
      "30/30 [==============================] - 0s 570us/step\n",
      "31/31 [==============================] - 0s 574us/step\n",
      "You walked for 45.9 minutes at a pace of 72 steps per minute on average\n",
      "You ran for 15.85 minutes at a pace of 148 steps per minute on average\n",
      "You biked for 16.28 minutes at a pace of 48 revolutions per minute on average\n"
     ]
    }
   ],
   "source": [
    "predictions = model_classify.predict(whitened_tf_sparse_train)\n",
    "still = []\n",
    "walk = []\n",
    "run = []\n",
    "bike = []\n",
    "for i, item in enumerate(predictions):\n",
    "    if np.argmax(item) == 0:\n",
    "        still.append(whitened_tf_sparse_train[i])\n",
    "    if np.argmax(item) == 1:\n",
    "        walk.append(whitened_tf_sparse_train[i])\n",
    "    if np.argmax(item) == 2:\n",
    "        run.append(whitened_tf_sparse_train[i])\n",
    "    if np.argmax(item) == 3:\n",
    "        bike.append(whitened_tf_sparse_train[i])\n",
    "walk_cad = model_walk.predict(np.array(walk))\n",
    "run_cad = model_run.predict(np.array(run))\n",
    "bike_cad = model_bike.predict(np.array(bike))\n",
    "walk_time = len(walk)/(60)\n",
    "walk_avg = np.mean(walk_cad)\n",
    "run_time = len(run)/(60)\n",
    "run_avg = np.mean(run_cad)\n",
    "bike_time = len(bike)/(60)\n",
    "bike_avg = np.mean(bike_cad)\n",
    "print(\"You walked for \"+str(round(walk_time,2))+\" minutes at a pace of \"+str(int(walk_avg))+\" steps per minute on average\")\n",
    "print(\"You ran for \"+str(round(run_time,2))+\" minutes at a pace of \"+str(int(run_avg))+\" steps per minute on average\")\n",
    "print(\"You biked for \"+str(round(bike_time,2))+\" minutes at a pace of \"+str(int(bike_avg))+\" revolutions per minute on average\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9333800841514727\n",
      "[[  1   0   0   0]\n",
      " [  0 529   7   0]\n",
      " [  2  60 233   0]\n",
      " [ 18   8   0 568]]\n",
      "[[1072    0    0    0]\n",
      " [   0 2958    0    0]\n",
      " [   0    0  943    0]\n",
      " [   0    0    0  818]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "randomForest = RandomForestClassifier()\n",
    "whitened_tf_sparse_train_flat = whitened_tf_sparse_train.reshape(whitened_tf_sparse_train.shape[0], -1)\n",
    "whitened_tf_sparse_test_flat = whitened_tf_sparse_test.reshape(whitened_tf_sparse_test.shape[0], -1)\n",
    "randomForest.fit(whitened_tf_sparse_train_flat, tf_labels_train[1:])\n",
    "train_predictions = randomForest.predict(whitened_tf_sparse_train_flat)\n",
    "predictions = randomForest.predict(whitened_tf_sparse_test_flat)\n",
    "accuracy = accuracy_score(tf_labels_test[1:], predictions)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "confusion = confusion_matrix(tf_labels_test[1:], predictions)\n",
    "print(confusion)\n",
    "confusion_train = confusion_matrix(tf_labels_train[1:], train_predictions)\n",
    "print(confusion_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4QSenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
